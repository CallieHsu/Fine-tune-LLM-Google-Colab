{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CallieHsu/Fine-tune-LLM-Google-Colab/blob/master/fine_tune_tinyllama_by_colab_docker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghURA2YjMDmw"
      },
      "source": [
        "# Tiny Llama Fine-Tuning using QLoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_dMlp0EXnTK"
      },
      "source": [
        "## 安裝所需套件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLXwJqbjtPho",
        "outputId": "54af9c4b-be78-4737-9520-387651fd4004"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m498.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m660.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m662.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.9/150.9 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ibis-framework 6.2.0 requires pyarrow<13,>=2, but you have pyarrow 15.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# 安裝所需的 Python 套件\n",
        "!pip install -q --upgrade pip\n",
        "!pip install -q accelerate==0.26.1 peft==0.7.1 bitsandbytes==0.42.0 transformers trl==0.7.10 huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check your Nvidia GPU"
      ],
      "metadata": {
        "id": "T_vl0b1JcaGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "79Or0cElH4X_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8df09f45-2dc4-4d42-f8ae-2f555db72372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Feb 20 08:49:06 2024       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.141.10   Driver Version: 470.141.10   CUDA Version: 12.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-DGXS...  On   | 00000000:07:00.0 Off |                    0 |\n",
            "| N/A   41C    P0    38W / 300W |     72MiB / 32508MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   1  Tesla V100-DGXS...  On   | 00000000:08:00.0 Off |                    0 |\n",
            "| N/A   40C    P0    38W / 300W |      5MiB / 32508MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   2  Tesla V100-DGXS...  On   | 00000000:0E:00.0 Off |                    0 |\n",
            "| N/A   40C    P0    36W / 300W |      5MiB / 32508MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   3  Tesla V100-DGXS...  On   | 00000000:0F:00.0 Off |                    0 |\n",
            "| N/A   41C    P0    38W / 300W |      5MiB / 32508MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAMzy_0FtaUZ"
      },
      "outputs": [],
      "source": [
        "# 匯入必要的模組和套件\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1Z0trJKXnTM"
      },
      "source": [
        "## 設定參數\n",
        "\n",
        "- model_name : 預訓練模型的名稱。\n",
        "- dataset_name : 訓練數據集的文件路徑。\n",
        "- new_model : 新模型的名稱。\n",
        "- lora_r, lora_alpha, lora_dropout : LoRA 的相關參數。\n",
        "- use_4bit : 是否使用 4 位定點數。\n",
        "- bnb_4bit_compute_dtype : 4 位定點數計算的數據類型。\n",
        "- bnb_4bit_quant_type : 4 位定點數的量化類型。\n",
        "- use_nested_quant : 是否使用嵌套量化。\n",
        "- output_dir : 輸出的目錄。\n",
        "- num_train_epochs : 訓練的週期數。\n",
        "- fp16 : 是否使用 16 位浮點數。\n",
        "- bf16 : 是否使用 bfloat16。\n",
        "- per_device_train_batch_size : 每個設備的訓練批次大小。\n",
        "- per_device_eval_batch_size : 每個設備的評估批次大小。\n",
        "- gradient_accumulation_steps : 梯度累積的步數。\n",
        "- gradient_checkpointing : 是否使用梯度檢查點。\n",
        "- max_grad_norm : 梯度的最大範數。\n",
        "- learning_rate : 學習速率。\n",
        "- weight_decay : 權重衰減。\n",
        "- optim : 優化器。\n",
        "- lr_scheduler_type : 學習速率的調整方式。\n",
        "- max_steps : 最大的訓練步數。\n",
        "- warmup_ratio : 學習速率的熱身比例。\n",
        "- group_by_length : 是否根據句子的長度將它們分組。\n",
        "- save_steps : 模型保存的步數。\n",
        "- logging_steps : 日誌記錄的步數。\n",
        "- max_seq_length : 輸入序列的最大長度。\n",
        "- packing : 是否打包序列。\n",
        "- device_map : 使用哪一個GPU。\n",
        "- 詳細參數：https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/trainer#transformers.TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ib_We3NLtj2E"
      },
      "outputs": [],
      "source": [
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v0.6\" # 欲訓練的HuggineFace model name\n",
        "new_model = \"tiny-llama-shuttle-xpc-cube-en5000\" # 新model的名稱\n",
        "\n",
        "################################################################################\n",
        "# Quantized LLMs with Low-Rank Adapters (QLoRA) parameters\n",
        "################################################################################\n",
        "lora_r = 64\n",
        "lora_alpha = 32\n",
        "lora_dropout = 0.1\n",
        "\n",
        "################################################################################\n",
        "# bitsandbytes parameters 輕量級封裝，專門用於CUDA自定義函數，特別是8位優化器、矩陣乘法和量化\n",
        "################################################################################\n",
        "use_4bit = True\n",
        "bnb_4bit_compute_dtype = \"float16\" # float16 or bfloat16\n",
        "bnb_4bit_quant_type = \"nf4\" # fp4 or nf4\n",
        "use_nested_quant = False\n",
        "\n",
        "################################################################################\n",
        "# TrainingArguments parameters\n",
        "################################################################################\n",
        "output_dir = \"./results\"\n",
        "num_train_epochs = 5000\n",
        "fp16 = False\n",
        "bf16 = False\n",
        "per_device_train_batch_size = 4\n",
        "per_device_eval_batch_size = 4\n",
        "gradient_accumulation_steps = 1\n",
        "gradient_checkpointing = True\n",
        "max_grad_norm = 0.3\n",
        "learning_rate = 2e-4\n",
        "weight_decay = 0.001\n",
        "optim = \"paged_adamw_32bit\"\n",
        "lr_scheduler_type = \"cosine\"\n",
        "max_steps = -1\n",
        "warmup_ratio = 0.03\n",
        "group_by_length = True\n",
        "save_steps = 50\n",
        "logging_steps = 50\n",
        "\n",
        "################################################################################\n",
        "# Supervised finetuning (SFT) parameters\n",
        "################################################################################\n",
        "max_seq_length = None\n",
        "packing = False\n",
        "device_map = {\"\": 0} #{\"\": 0} or \"auto\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djP1yz5UXnTN"
      },
      "source": [
        "## 讀取資料集 & 前處理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5N4L43tEhgO"
      },
      "source": [
        "Tiny-llama\n",
        "訓練用chat prompt template格式:\n",
        "\n",
        "```<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n{answer}<|im_end|>\\n```\n",
        "\n",
        "問答資料預先整理成JSON格式, example:\n",
        "```\n",
        "{\"input\": \"What models are part of the Shuttle XPC cube series?\", \"output\": \"The Shuttle XPC cube series includes SH610R4, SW580R8, SH510R4, SH570R8, SH570R6, SH370R6 V2, SH310R4 V2, SH370R8, SH310R4, SH370R6, SZ270R9, SZ270R8.\"}\n",
        "{\"input\": \"Can you list the different models in the Shuttle XPC cube series?\", \"output\": \"The Shuttle XPC cube series comprises SH610R4, SW580R8, SH510R4, SH570R8, SH570R6, SH370R6 V2, SH310R4 V2, SH370R8, SH310R4, SH370R6, SZ270R9, SZ270R8.\"}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYFLaAcoK7Zi"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset_file = './shuttle-xpc-cube-en.jsonl'\n",
        "\n",
        "def format_chat(prompt, response):\n",
        "    return f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n{response}<|im_end|>\\n\"\n",
        "\n",
        "def preprocess_data(data_entry):\n",
        "    return {'text': [format_chat(*entry) for entry in zip(data_entry['input'], data_entry['output'])]}\n",
        "\n",
        "# 讀取資料集\n",
        "ds = load_dataset('json', data_files=dataset_file, split=\"train\")\n",
        "\n",
        "# 分割成 train: 90%, test: 10%\n",
        "ds_train_valid = ds.train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "# batched=True -> 允許使用map一次套用\n",
        "train_dataset = ds_train_valid['train'].map(preprocess_data, batched=True)\n",
        "valid_dataset = ds_train_valid['test'].map(preprocess_data, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bz-H90st34O"
      },
      "source": [
        "## 下載模型及微調模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJXpOgBFuSrc"
      },
      "outputs": [],
      "source": [
        "# 定義位元和字節量化的相關配置\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "# 檢查 GPU 是否與 bfloat16 相容\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# 從預訓練模型中載入自動生成模型\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "# 載入與模型對應的分詞器\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
        "\n",
        "# 定義 Prompt Engineering Fine-Tuning （PEFT）的相關設定\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# 設置訓練參數\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"tensorboard\", #\"all\"\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=5,  # 每5部驗證\n",
        "    load_best_model_at_end=True #將最佳評估結果的模型讀出來\n",
        ")\n",
        "\n",
        "# 使用 SFTTrainer 進行監督式微調訓練\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset, # 在這裡傳入驗證數據集\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=packing,\n",
        ")\n",
        "\n",
        "# 開始訓練模型\n",
        "trainer.train()\n",
        "\n",
        "# 儲存微調後的模型\n",
        "trainer.model.save_pretrained(new_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crj9svNe4hU5"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir results/runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUQiSTXnLBj6"
      },
      "source": [
        "## 建立chat prompt模板"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiXEiYt983zz"
      },
      "outputs": [],
      "source": [
        "# def formatted_prompt(question)-> str:\n",
        "#     return f\"### user:\\n{question}\\n### assistant:\\n\"\n",
        "\n",
        "def template_prompt(question)-> str:\n",
        "    # 參考自transformers的apply_chat_template api的輸出格式\n",
        "    prompt = f\"\"\"<|system|>\n",
        "Below is an instruction that describes a task. Write a response that appropriately completes the request.</s>\n",
        "<|user|>\n",
        "{question}</s>\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFW5gI74quPm"
      },
      "source": [
        "## 模型合併並儲存\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-xPb-_qB0dz",
        "outputId": "164dfa83-8b84-4a51-a549-9a18a46fadbf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./tiny-llama-shuttle-xpc-cube-en2000/tokenizer_config.json',\n",
              " './tiny-llama-shuttle-xpc-cube-en2000/special_tokens_map.json',\n",
              " './tiny-llama-shuttle-xpc-cube-en2000/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "model_path = new_model  # 更改為您的路徑\n",
        "\n",
        "# 以FP16重新載入模型並將其與LoRA權重合併\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=device_map,\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, new_model)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# 重新載入分詞器以進行保存\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# 儲存合併後的模型\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuaC_lwc_kWp"
      },
      "outputs": [],
      "source": [
        "# Fix unicode problem in Colab\n",
        "import locale\n",
        "print(locale.getpreferredencoding())\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 上傳至HuggingFace"
      ],
      "metadata": {
        "id": "CjXmi9eAKBvc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ev-LlD-o7UDf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "512dc2e5-a03b-4f6e-a17f-03eab339c6af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "# make sure using your WRITE token\n",
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "4f0c287571904e2f8cbf4081760a18e8",
            "31ef56577b1a4b2a8df9e5b7030eb9c1",
            "752b8189a1da4b0794671cf0026dc968",
            "3f68467288b6489887924b83d6da6e52",
            "23389da5f2534475aa857a4237022941",
            "b0a5b84575a9469687f9e190f62a71c1",
            "aea276a86ac242089934d8f00b33a556",
            "c4554a3f1cc745978b2dca8c67253f45",
            "19003c27708341f0a8a87b0960b58181",
            "d63f3072172047ef81f213d7bee20cfc",
            "13ebaa08aba24fe7acc40ad8235c2468"
          ]
        },
        "id": "PMS3eHbC8gY4",
        "outputId": "30ed2d0d-e5ca-4534-aa8a-af0531a60ab0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "calliehsu\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f0c287571904e2f8cbf4081760a18e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/calliehsu/tiny-llama-shuttle-xpc-cube-en2000/commit/16db04d7c59560dba7ad7c286a12201f594cb2a4', commit_message='Upload tokenizer', commit_description='', oid='16db04d7c59560dba7ad7c286a12201f594cb2a4', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "!huggingface-cli whoami\n",
        "\n",
        "model.push_to_hub(new_model, use_temp_dir=False)\n",
        "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYkMns8QZxKR"
      },
      "source": [
        "# 載入微調後的模型並執行推論"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12drw4RIAf4q"
      },
      "source": [
        "### Fine tune model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B355-o6rZxn7"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "model_path = new_model\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path,\n",
        "                         device_map=\"auto\",\n",
        "                         offload_folder=\"offload\",\n",
        "                         torch_dtype=torch.float16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtXtREf-Zza5"
      },
      "outputs": [],
      "source": [
        "prompts = [\n",
        "    \"Provide the names of the models within the Shuttle XPC cube series.\",\n",
        "    \"What is the processor support for the Shuttle XPC cube SH610R4?\",\n",
        "    \"Which generation of Intel processors does the Shuttle XPC cube SW580R8 support?\",\n",
        "    \"What are the CPU choices for the Shuttle XPC cube SH510R4?\",\n",
        "    \"Tell me about the chipset used in the Shuttle XPC cube SH570R8.\",\n",
        "    \"What are the dimensions of the Shuttle XPC cube SZ270R9?\",\n",
        "    \"What is the maximum power output of the power supply in the Shuttle XPC cube SH310R4 V2?\",\n",
        "    \"Please provide me the comparison of CPU specifications of Shuttle xpc cube series.\"\n",
        "]\n",
        "\n",
        "gen = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "for prompt in prompts:\n",
        "    print(\"Q: \", prompt)\n",
        "    prompt = template_prompt(prompt)\n",
        "\n",
        "    result = gen(\n",
        "        prompt,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        max_new_tokens=256,\n",
        "    )\n",
        "    print(\"A: \", result[0]['generated_text'].split(\"<|assistant|>\")[-1].strip(), \"\\n\") # 輸出生成的文本"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JwwHe0dAlDR"
      },
      "source": [
        "### Raw pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpPEQRv3_hum"
      },
      "outputs": [],
      "source": [
        "# 從預訓練模型中載入自動生成模型\n",
        "model0 = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "model0.config.use_cache = False\n",
        "\n",
        "# 載入與模型對應的分詞器\n",
        "tokenizer0 = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer0.pad_token = tokenizer0.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcHcYjsJ_xgp"
      },
      "outputs": [],
      "source": [
        "prompts = [\n",
        "    \"Shuttle xpc cube系列有哪些型號?\",\n",
        "    \"SH610R4的規格是?\",\n",
        "    \"SH510R4的技術規格?\",\n",
        "    \"SH370R6的規格是什麼?\",\n",
        "    \"SH370R6支援哪些CPU?\",\n",
        "    \"SH570R8的操作溫度範圍是多少?\",\n",
        "    \"SH370R6 V2的規格是什麼?\",\n",
        "    \"SZ270R9的尺寸是多少?\",\n",
        "]\n",
        "\n",
        "pipe = pipeline(task=\"text-generation\", model=model0, tokenizer=tokenizer0)\n",
        "for prompt in prompts:\n",
        "    print(\"Q: \", prompt)\n",
        "    prompt = template_prompt(prompt)\n",
        "\n",
        "    result = pipe(\n",
        "        prompt,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer0.eos_token_id,\n",
        "        max_new_tokens=128,\n",
        "        repetition_penalty=1.1\n",
        "    )\n",
        "    print(\"A: \", result[0]['generated_text'].split(\"<|assistant|>\")[-1].strip(), \"\\n\") # 輸出生成的文本"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAnZm87GaJS4"
      },
      "source": [
        "**code reference:**\n",
        "- Llama2 finetune: https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4f0c287571904e2f8cbf4081760a18e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_31ef56577b1a4b2a8df9e5b7030eb9c1",
              "IPY_MODEL_752b8189a1da4b0794671cf0026dc968",
              "IPY_MODEL_3f68467288b6489887924b83d6da6e52"
            ],
            "layout": "IPY_MODEL_23389da5f2534475aa857a4237022941"
          }
        },
        "31ef56577b1a4b2a8df9e5b7030eb9c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0a5b84575a9469687f9e190f62a71c1",
            "placeholder": "​",
            "style": "IPY_MODEL_aea276a86ac242089934d8f00b33a556",
            "value": "model.safetensors: 100%"
          }
        },
        "752b8189a1da4b0794671cf0026dc968": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4554a3f1cc745978b2dca8c67253f45",
            "max": 2200119664,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_19003c27708341f0a8a87b0960b58181",
            "value": 2200119664
          }
        },
        "3f68467288b6489887924b83d6da6e52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d63f3072172047ef81f213d7bee20cfc",
            "placeholder": "​",
            "style": "IPY_MODEL_13ebaa08aba24fe7acc40ad8235c2468",
            "value": " 2.20G/2.20G [17:40&lt;00:00, 2.59MB/s]"
          }
        },
        "23389da5f2534475aa857a4237022941": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0a5b84575a9469687f9e190f62a71c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aea276a86ac242089934d8f00b33a556": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4554a3f1cc745978b2dca8c67253f45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19003c27708341f0a8a87b0960b58181": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d63f3072172047ef81f213d7bee20cfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13ebaa08aba24fe7acc40ad8235c2468": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}